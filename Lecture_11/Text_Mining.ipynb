{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining in Python\n",
    "#### In our last lecture we learned about the bag of words (BOW) representation for transforming unstructured text into a document-term matrix that we could use with machine learning algorithms. Today, we'll learn about another way of representing the presence of terms in a document by reweighting the counts based on the importance of the terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf):<br>\n",
    "\n",
    "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf-idf value increases proportionally to the number of times a word appears in the document and is offset by the frequency of the word in the corpus, which helps to adjust for the fact that some words appear more frequently in general. Nowadays, tf-idf is one of the most popular term-weighting schemes; 83% of text-based recommender systems in the domain of digital libraries use tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Term Frequency: the number of times a term (token, e.g. a word) appears in a document<br>Document Frequency: the number of documents that a word appears in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's stop and think about this for a second. Say our goal is to find all relevant documents from a corpus given a search phrase. Say we're only allowed to search for documents using one word of the search phrase at a time. You've created a dictionary where all of the terms present in the documents are the keys and the values are set of tuples (document, term frequency). We'll need to consider each of the words in the search phrase to determine the relevance of each of the documents found.<br>\n",
    "\n",
    "example search phrase: <em>the fast fourier transform</em><br>\n",
    "\n",
    "Your first thought might be to take the intersection of each set of documents that contain each word. But how would you go about ordering those results? What if there was a document where French version of Pinocchio about a doll named Fourier that wanted to transform into a real boy (and do so fast)? How would you determine that was irrelevant?\n",
    "\n",
    "Let's consider each word:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>the</b>: this word probably appears in every document so a document containing <em>the</em> doesn't mean that it's relevant at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>fast</b>: this word probably appears in a lot of documents that have nothing to do with the fourier transform in addition to those about the fast fourier transform so it's not as useless as the but still pretty irrelevant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>fourier</b>: this word will appear in a lot less documents than the word fast, therefore it should be more relevant to our query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>transform</b>: this word will appear in more documents that <b>fourier</b> but less documents than <b>fast</b> and it's relevance should reflect that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also care how many times the word is mentioned. In a document about the fast fourier transform, we would expect each of those words to occur frequently. However, we should keep in mind that we care more about the relative frequency than the overall frequency. Therefore we should normalize the term frequency based on how many words are present in the document. So that we don't place assign higher relevance to a document merely because it is longer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, that the relevance of any document is directly proportional to the normalized term frequency and inversely proportional to how many documents the term appears in. This is the motivation behind tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we're looking for something like:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{tf-idf} = f(\\text{term freq}) \\times g({\\frac{1}{\\text{doc freq}}})\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which could be as simple as:\n",
    "\n",
    "\\begin{equation*}\n",
    "\\text{tf-idf}_{word} = \\langle\\text{term freq}_{word}\\rangle \\times log \\left( \\frac{N_{doc}}{\\text{doc freq}_{word}} \\right)\n",
    "\\end{equation*}\n",
    "\n",
    "using the log for smaller overall values in case $N_{doc}$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various calculations used for calculating the tf-idf score. The Wikipedia page lists several. Refer to the Scikit documenation to see which one they use and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sentences = ['The dog is a good dog.', \n",
    "                     'The boy is bad.', \n",
    "                     'The girl is good.',\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 2., 0., 1.],\n",
       "       [1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 1.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, norm=None, stop_words='english', use_idf=False)\n",
    "tfidf.fit_transform(example_sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bad', 'boy', 'dog', 'girl', 'good']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.66666667, 0.        , 0.33333333],\n",
       "       [0.5       , 0.5       , 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.5       , 0.5       ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, norm='l1', stop_words='english', use_idf=False)\n",
    "tfidf.fit_transform(example_sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 3.38629436, 0.        , 1.28768207],\n",
       "       [1.69314718, 1.69314718, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 1.69314718, 1.28768207]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, norm=None, stop_words='english', use_idf=True)\n",
    "tfidf.fit_transform(example_sentences).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.93470196, 0.        , 0.35543247],\n",
       "       [0.70710678, 0.70710678, 0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.79596054, 0.60534851]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(lowercase=True, norm='l2', stop_words='english', use_idf=True)\n",
    "tfidf.fit_transform(example_sentences).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [Scikit-learn docs](http://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting):\n",
    "> In a large text corpus, some words will be very present (e.g. “the”, “a”, “is” in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms.<br><br>\n",
    "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learn more about tf-idf: <br> http://blog.christianperone.com/2011/09/machine-learning-text-feature-extraction-tf-idf-part-i/ <br> http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also learned in the previous lecture about n-grams and that one of the problems with calculating n-grams is that our number of features will explode. What if we came up with a way to identify meaningful/significant n-grams and only used those instead. Lucky for us, some people already figured out some ways to do just that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From [Wikipedia](https://en.wikipedia.org/wiki/Collocation):\n",
    "\n",
    "In corpus linguistics, a collocation is a sequence of words or terms that co-occur more often than would be expected by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I won't go into the details of the calculations here. But if you would like to work collocations in your project, here are resources to learn more about them:<br>\n",
    "https://nlp.stanford.edu/fsnlp/promo/colloc.pdf <br>\n",
    "http://www.scielo.org.mx/scielo.php?pid=S1405-55462016000300327&script=sci_arttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from spacy.pipeline import Pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now instead of just using the frequency of the word in the document. You're reweighting the frequency based on how important that term should be based on the tfidf score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = pd.read_csv('../Lecture_10/labeledTrainData.tsv/labeledTrainData.tsv', sep='\\t')\n",
    "text = movie_data.sample(10000, random_state=42).loc[:, 'review'].apply(lambda t: bs4.BeautifulSoup(t, 'lxml').get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: using lxml instead of html5lib will significantly speed up the html parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I read that \\There's a Girl in My Soup\\\" came out during Peter Sellers's low period. Watching the movie, I'm not surprised. Almost nothing happens in the movie. Seemingly, the very presence of Sellers and Goldie Hawn should help the movie; it doesn't. The whole movie seems like they just randomly filmed whatever happened without scripting anything. Maybe I haven't seen every movie about middle-aged to elderly people trying to be hippies, but this one gives such movies a pretty bad name.All in all, both Sellers and Hawn have starred in much better movies than this, so don't waste your time on this. Pretty worthless.\"\n"
     ]
    }
   ],
   "source": [
    "print(text.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's find collocations at the sentence level instead of the review level so we don't find collocations between words at the end of sentences and the beginning of others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10min 18s, sys: 3min 10s, total: 13min 29s\n",
      "Wall time: 8min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "token_text = []\n",
    "\n",
    "for doc in nlp.pipe(text):\n",
    "    for sent in doc.sents:\n",
    "        token_text.append([t.lower_ for t in sent if not t.is_punct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'read', 'that', '\\\\there', \"'s\", 'a', 'girl', 'in', 'my', 'soup\\\\', 'came', 'out', 'during', 'peter', 'sellers', \"'s\", 'low', 'period']\n"
     ]
    }
   ],
   "source": [
    "print(token_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_terms = list(stop_words.ENGLISH_STOP_WORDS) + [\"'m\", \"'re\", \"'ll\", \"'s\", \"'ve\", \"'d\", 'ca', 'is']\n",
    "\n",
    "common_terms.remove('not')\n",
    "common_terms.remove('nothing')\n",
    "common_terms.remove('never')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amoungst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'bill',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'cant',\n",
       " 'co',\n",
       " 'con',\n",
       " 'could',\n",
       " 'couldnt',\n",
       " 'cry',\n",
       " 'de',\n",
       " 'describe',\n",
       " 'detail',\n",
       " 'do',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eg',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'etc',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'fill',\n",
       " 'find',\n",
       " 'fire',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'found',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'hasnt',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'ie',\n",
       " 'if',\n",
       " 'in',\n",
       " 'inc',\n",
       " 'indeed',\n",
       " 'interest',\n",
       " 'into',\n",
       " 'is',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'ltd',\n",
       " 'made',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mill',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'rather',\n",
       " 're',\n",
       " 'same',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'sincere',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'system',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'un',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(common_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases = Phrases(token_text, common_terms=common_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "colloc = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "colloc_text = colloc[token_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'read',\n",
       " 'that',\n",
       " '\\\\there',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'girl',\n",
       " 'in',\n",
       " 'my',\n",
       " 'soup\\\\',\n",
       " 'came',\n",
       " 'out',\n",
       " 'during',\n",
       " 'peter_sellers',\n",
       " \"'s\",\n",
       " 'low',\n",
       " 'period']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colloc_text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'unassuming',\n",
       " 'subtle',\n",
       " 'and',\n",
       " 'lean',\n",
       " 'film',\n",
       " '\\\\the',\n",
       " 'man',\n",
       " 'in',\n",
       " 'the',\n",
       " 'white',\n",
       " 'suit\\\\',\n",
       " 'is',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'breath_of_fresh',\n",
       " 'air',\n",
       " 'in',\n",
       " 'filmic',\n",
       " 'format',\n",
       " 'from',\n",
       " 'ealing',\n",
       " 'studios']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colloc_text[1999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_phrases = Phrases(colloc_text, common_terms=common_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_colloc = Phraser(tri_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_colloc_text = tri_colloc[colloc_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an',\n",
       " 'unassuming',\n",
       " 'subtle',\n",
       " 'and',\n",
       " 'lean',\n",
       " 'film',\n",
       " '\\\\the',\n",
       " 'man',\n",
       " 'in',\n",
       " 'the',\n",
       " 'white',\n",
       " 'suit\\\\',\n",
       " 'is',\n",
       " 'yet',\n",
       " 'another',\n",
       " 'breath_of_fresh_air',\n",
       " 'in',\n",
       " 'filmic',\n",
       " 'format',\n",
       " 'from',\n",
       " 'ealing',\n",
       " 'studios']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_colloc_text[1999]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now you can use this Phraser to convert a list of tokens into a list of tokens that groups together collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it', 'was', 'a', 'waste_of_time']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tri_colloc[['it', 'was', 'a', 'waste', 'of', 'time']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the gensim docs for more info: https://radimrehurek.com/gensim/models/phrases.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some of your projects, your goal is to figure out the sentiment expressed on specific aspects of an object. In order to do that, you'd have to account for all of the different ways a person could refer to that aspect.\n",
    "\n",
    "Say you're looking at product reviews for cell phones and you've noticed one aspect of cell phones that reviewers seem to care about is the battery life. But you've noticed that sometimes they talk about that aspect using different words such as: 'battery life', 'battery', and 'battery power.' You now know how to find collocations such as 'battery_life' and 'battery_power.' But how would know that those are all used to refer to the same thing. This is an unsupervised learning problem. You don't have labels for each of those terms telling you that they refer to \"battery life.\" So you need a way to learn from the text that those terms are used to refer to the same aspect. Word2Vec can do this for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://deeplearning4j.org/img/word2vec_diagrams.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gist:\n",
    "\n",
    "By using surrounding words (context) to predict a word or by using a word to predict the surrounding words, you can use the hidden layer of the NN to map words to a lower dimensional vector space (instead of the original vector space that had the same number of dimensions as the number of words in your corpus vocabulary). In order to shrink the vector space, the NN has to learn to recognize patterns in the text (represenatations) to compress the information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you get from word2vec are vectors for each word where the position of word in the lower dimensional vector space represents some concept and similar words (words used in similar contexts in the training data) are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these vectors, you can cluster the words together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 1.93 s, total: 1min 27s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model = Word2Vec(tri_colloc_text, size=100, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.08353972e-01,  1.40819073e-01,  3.88194174e-01,  1.96526852e-03,\n",
       "       -8.26176286e-01, -2.41572540e-02,  3.40961754e-01, -1.12723410e+00,\n",
       "       -1.24302141e-01, -6.08248949e-01,  3.92841041e-01,  1.13429093e+00,\n",
       "       -6.65594101e-01, -7.96273947e-01, -1.73878884e+00,  1.49672046e-01,\n",
       "        2.32765928e-01,  5.76645970e-01,  7.62504876e-01,  1.84046459e+00,\n",
       "       -7.46206045e-01,  6.51902080e-01,  1.16231477e+00,  3.65196377e-01,\n",
       "        1.46791473e-01,  4.28103358e-01,  4.50814366e-01,  3.60439390e-01,\n",
       "        1.19752860e+00, -1.00293875e+00, -2.06143737e+00,  9.02431756e-02,\n",
       "        3.82894397e-01,  5.03197089e-02, -1.72153533e-01,  2.12108687e-01,\n",
       "        1.50363934e+00, -1.40860423e-01,  9.75907803e-01,  1.31152973e-01,\n",
       "       -3.28705341e-01,  7.03209937e-01,  8.88488218e-02,  3.74101818e-01,\n",
       "       -1.25820339e+00,  1.34960458e-01, -6.26231432e-01,  7.85148263e-01,\n",
       "        1.02406454e+00,  1.19477010e+00,  5.92695594e-01,  1.05428958e+00,\n",
       "       -1.82161462e-02, -9.93636966e-01, -1.60642135e+00,  4.60966630e-03,\n",
       "       -7.70626247e-01,  7.49970138e-01, -9.53008115e-01,  4.43287879e-01,\n",
       "       -8.07159692e-02, -3.85070205e-01,  5.74482307e-02,  3.15227747e-01,\n",
       "        1.67559266e+00,  4.32471186e-02,  1.57026684e+00, -3.97310346e-01,\n",
       "       -9.03278649e-01,  1.52204752e-01, -9.44833815e-01, -9.19560373e-01,\n",
       "       -6.95320427e-01, -7.18670338e-02, -1.12362675e-01,  9.58005071e-01,\n",
       "        3.01418930e-01,  3.15794140e-01,  8.78160596e-01, -5.01048528e-02,\n",
       "        1.22472632e+00,  4.64202344e-01, -3.42145979e-01, -1.66559148e+00,\n",
       "       -7.99475133e-01, -1.45033669e+00,  4.89861399e-01, -6.68107033e-01,\n",
       "       -8.58012676e-01, -2.11450964e-01, -7.17233062e-01,  2.41632447e-01,\n",
       "       -2.10946321e-01, -6.94269687e-02, -9.78700936e-01, -7.40100920e-01,\n",
       "       -2.51721472e-01, -1.01380384e+00, -4.58970189e-01, -6.81381822e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv['cinematography']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('photography', 0.9435670971870422),\n",
       " ('camera_work', 0.9348364472389221),\n",
       " ('lighting', 0.9125988483428955),\n",
       " ('direction', 0.9095066785812378),\n",
       " ('music', 0.909207284450531),\n",
       " ('soundtrack', 0.8958258032798767),\n",
       " ('editing', 0.8878849148750305),\n",
       " ('pacing', 0.8850436210632324),\n",
       " ('scenery', 0.8823689818382263),\n",
       " ('dialog', 0.8796185255050659)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('cinematography')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('storyline', 0.8430126905441284),\n",
       " ('story', 0.8208246231079102),\n",
       " ('story_line', 0.8002089262008667),\n",
       " ('dialogue', 0.7878210544586182),\n",
       " ('script', 0.7815348505973816),\n",
       " ('ending', 0.7605075836181641),\n",
       " ('concept', 0.7239733934402466),\n",
       " ('message', 0.7239457964897156),\n",
       " ('dialog', 0.7191160917282104),\n",
       " ('sound', 0.707491934299469)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('role', 0.73625248670578),\n",
       " ('performance', 0.7231549620628357),\n",
       " ('voice', 0.6991214156150818),\n",
       " ('villain', 0.6930806040763855),\n",
       " ('main_character', 0.682639479637146),\n",
       " ('portrayal', 0.6545627117156982),\n",
       " ('situation', 0.6361498832702637),\n",
       " ('presence', 0.6356627941131592),\n",
       " ('relationship', 0.6308934092521667),\n",
       " ('actor', 0.6215326189994812)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('character')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('writer', 0.7605269551277161),\n",
       " ('actor', 0.6439990401268005),\n",
       " ('filmmaker', 0.641372561454773),\n",
       " ('screenplay', 0.5950239896774292),\n",
       " ('author', 0.5923200845718384),\n",
       " ('cast', 0.5767263770103455),\n",
       " ('producer', 0.5612565875053406),\n",
       " ('casting', 0.5563101768493652),\n",
       " ('performance', 0.5456812381744385),\n",
       " ('role', 0.5445694923400879)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('director')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for sent in tri_colloc_text:\n",
    "    vocab_set.update(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = pd.Series(list(model.wv.vocab))\n",
    "vocab_vectors = []\n",
    "\n",
    "for word in vocab:\n",
    "    try:\n",
    "        vec = model.wv[word]\n",
    "        vocab_vectors.append(vec)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22838, 22838)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), len(vocab_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02460557,  0.47760966, -2.9226177 ,  0.13891673,  0.6100589 ,\n",
       "       -0.09417142, -2.1427715 , -0.39144024, -0.25622603, -1.268989  ,\n",
       "       -1.6281679 ,  0.0811056 ,  1.0130559 , -0.60660034,  0.571093  ,\n",
       "       -1.9916303 , -2.333888  ,  2.3250675 , -0.9635863 ,  1.505168  ,\n",
       "       -2.632644  ,  3.0234292 ,  0.23095961, -2.334893  ,  0.4685145 ,\n",
       "       -2.6683495 , -1.2293535 , -1.0694523 , -0.93073964,  0.43642136,\n",
       "       -1.4611065 , -2.4278991 , -1.8014826 , -0.4825394 , -0.21139567,\n",
       "       -1.0011889 , -1.2318618 , -0.7314823 , -0.25001726,  1.7385334 ,\n",
       "       -0.31799376,  0.4863861 , -0.7905135 , -1.7888017 ,  0.523053  ,\n",
       "        0.46604916,  0.67809457, -2.8256426 , -0.08586459, -3.0361795 ,\n",
       "        1.0235425 , -1.2746819 , -3.718552  , -1.9863592 , -0.6691114 ,\n",
       "       -1.0657921 , -2.314538  ,  4.4069295 , -0.4689855 ,  0.616245  ,\n",
       "       -1.0528014 ,  1.9347531 ,  0.9588505 ,  3.0499837 ,  0.03572845,\n",
       "        0.18023825, -0.51651186,  1.2198809 ,  1.1035601 ,  2.1307776 ,\n",
       "        2.1414692 ,  1.6351578 , -1.202176  , -0.6480848 , -1.8996004 ,\n",
       "        0.42580116, -1.613199  ,  1.1180195 , -0.45449972,  0.77883375,\n",
       "       -1.2223476 , -0.8647623 ,  4.7999606 , -0.87316054,  0.8012587 ,\n",
       "        2.0027902 , -1.4520916 , -0.7320335 ,  2.1912231 , -1.464934  ,\n",
       "        1.2039533 , -0.7074355 ,  2.1530015 ,  1.5729797 ,  1.6950572 ,\n",
       "       -0.17397866, -3.0125496 ,  1.2452106 , -2.0397403 , -1.0826778 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_vectors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_array = np.concatenate(vocab_vectors, axis=0).reshape(-1, 100)\n",
    "\n",
    "vec_array_l1 = normalize(vector_array, norm='l1')\n",
    "vec_array_l2 = normalize(vector_array, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02460557,  0.47760966, -2.9226177 ,  0.13891673,  0.6100589 ,\n",
       "       -0.09417142, -2.1427715 , -0.39144024, -0.25622603, -1.268989  ,\n",
       "       -1.6281679 ,  0.0811056 ,  1.0130559 , -0.60660034,  0.571093  ,\n",
       "       -1.9916303 , -2.333888  ,  2.3250675 , -0.9635863 ,  1.505168  ,\n",
       "       -2.632644  ,  3.0234292 ,  0.23095961, -2.334893  ,  0.4685145 ,\n",
       "       -2.6683495 , -1.2293535 , -1.0694523 , -0.93073964,  0.43642136,\n",
       "       -1.4611065 , -2.4278991 , -1.8014826 , -0.4825394 , -0.21139567,\n",
       "       -1.0011889 , -1.2318618 , -0.7314823 , -0.25001726,  1.7385334 ,\n",
       "       -0.31799376,  0.4863861 , -0.7905135 , -1.7888017 ,  0.523053  ,\n",
       "        0.46604916,  0.67809457, -2.8256426 , -0.08586459, -3.0361795 ,\n",
       "        1.0235425 , -1.2746819 , -3.718552  , -1.9863592 , -0.6691114 ,\n",
       "       -1.0657921 , -2.314538  ,  4.4069295 , -0.4689855 ,  0.616245  ,\n",
       "       -1.0528014 ,  1.9347531 ,  0.9588505 ,  3.0499837 ,  0.03572845,\n",
       "        0.18023825, -0.51651186,  1.2198809 ,  1.1035601 ,  2.1307776 ,\n",
       "        2.1414692 ,  1.6351578 , -1.202176  , -0.6480848 , -1.8996004 ,\n",
       "        0.42580116, -1.613199  ,  1.1180195 , -0.45449972,  0.77883375,\n",
       "       -1.2223476 , -0.8647623 ,  4.7999606 , -0.87316054,  0.8012587 ,\n",
       "        2.0027902 , -1.4520916 , -0.7320335 ,  2.1912231 , -1.464934  ,\n",
       "        1.2039533 , -0.7074355 ,  2.1530015 ,  1.5729797 ,  1.6950572 ,\n",
       "       -0.17397866, -3.0125496 ,  1.2452106 , -2.0397403 , -1.0826778 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_array[0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.207269"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_array_l1[5].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=1000, init='random', n_jobs=1, max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 36s, sys: 11.2 s, total: 1min 48s\n",
      "Wall time: 1min 48s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='random', max_iter=1000,\n",
       "    n_clusters=1000, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "km.fit(vec_array_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "576    137\n",
       "237    108\n",
       "379    106\n",
       "508    103\n",
       "794    103\n",
       "280     97\n",
       "54      97\n",
       "506     89\n",
       "20      89\n",
       "136     85\n",
       "221     83\n",
       "269     81\n",
       "691     79\n",
       "499     77\n",
       "662     76\n",
       "480     76\n",
       "617     74\n",
       "17      73\n",
       "358     73\n",
       "582     73\n",
       "498     73\n",
       "6       71\n",
       "873     71\n",
       "900     69\n",
       "93      69\n",
       "843     68\n",
       "981     68\n",
       "309     68\n",
       "664     67\n",
       "92      67\n",
       "      ... \n",
       "401      1\n",
       "584      1\n",
       "552      1\n",
       "668      1\n",
       "777      1\n",
       "303      1\n",
       "232      1\n",
       "255      1\n",
       "960      1\n",
       "120      1\n",
       "88       1\n",
       "24       1\n",
       "896      1\n",
       "985      1\n",
       "130      1\n",
       "471      1\n",
       "439      1\n",
       "111      1\n",
       "327      1\n",
       "95       1\n",
       "167      1\n",
       "362      1\n",
       "71       1\n",
       "514      1\n",
       "594      1\n",
       "926      1\n",
       "486      1\n",
       "779      1\n",
       "470      1\n",
       "475      1\n",
       "Length: 1000, dtype: int64"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "pd.value_counts(km.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163               plot\n",
       " 223          character\n",
       " 1018    cinematography\n",
       " 2337         filmmaker\n",
       " dtype: object, array([286, 188, 694, 752], dtype=int32))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_lookup = ['cinematography', 'plot', 'character', 'filmmaker']\n",
    "series_filter = vocab.isin(words_to_lookup)\n",
    "word_labels = km.labels_[series_filter]\n",
    "words = vocab[series_filter]\n",
    "words, word_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============plot=============\n",
      "92               story\n",
      "107           material\n",
      "163               plot\n",
      "421            message\n",
      "598             comedy\n",
      "714         story_line\n",
      "1166             piece\n",
      "1779           quality\n",
      "1865           concept\n",
      "1976          thriller\n",
      "2637             drama\n",
      "3487         storyline\n",
      "3871            effect\n",
      "4410    subject_matter\n",
      "4428           premise\n",
      "dtype: object\n",
      "============character=============\n",
      "223          character\n",
      "1190             voice\n",
      "6730             truth\n",
      "7806    main_character\n",
      "dtype: object\n",
      "============cinematography=============\n",
      "133            direction\n",
      "164                 cast\n",
      "205                music\n",
      "258               acting\n",
      "501         performances\n",
      "545           soundtrack\n",
      "712           screenplay\n",
      "747             dialogue\n",
      "987     rest_of_the_cast\n",
      "1006             editing\n",
      "1018      cinematography\n",
      "1088              dialog\n",
      "1403         photography\n",
      "1441              script\n",
      "1525             scenery\n",
      "2295               sound\n",
      "2298               score\n",
      "2658     supporting_cast\n",
      "2723             effects\n",
      "2864     special_effects\n",
      "3230           directing\n",
      "3768             writing\n",
      "3991         camera_work\n",
      "4142             casting\n",
      "4430        storytelling\n",
      "4514              pacing\n",
      "4991           animation\n",
      "6053            lighting\n",
      "dtype: object\n",
      "============filmmaker=============\n",
      "174     performance\n",
      "1084          actor\n",
      "1089       director\n",
      "1507        actress\n",
      "1623         genius\n",
      "1767           role\n",
      "2154        villain\n",
      "2337      filmmaker\n",
      "2541         writer\n",
      "4401      portrayal\n",
      "6082         author\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "for cluster_num, word in zip(word_labels, words):\n",
    "    print('============{}============='.format(word))\n",
    "    print(vocab[km.labels_ == cluster_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dimensionality Reduction and Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [Wikipedia page](https://en.wikipedia.org/wiki/Singular-value_decomposition):<br>\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/e/e9/Singular_value_decomposition.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"font-size: 200%;\">$$\\mathbf{M} =\\mathbf{U} {\\boldsymbol {\\Sigma}}\\mathbf{V}^{*}$$</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read these:<br>\n",
    "http://www.ams.org/publicoutreach/feature-column/fcarc-svd<br>\n",
    "https://www.quora.com/What-is-an-intuitive-explanation-of-singular-value-decomposition-SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we are trying to compress the information stored in the N-dimensional data matrix down to a k-dimensional form. $(N < K)$ In order to accomplish that, we assume that each of the N column vectors of the original matrix can be represented as a linear combination of K vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does this help us with a BOW matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can think instead of having a column for each word. We can having columns that represent relationships between words and by summing them in the right way get back most of the information contained in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (Latent Semantic Indexing)\n",
    "http://matpalm.com/lsa_via_svd/intro.html<br>\n",
    "https://web.archive.org/web/20150823005532/http://www.puffinwarellc.com:80/index.php/news-and-articles/articles/33.html?start=3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new set of latent features to describe the composition of a document. By grouping words into concepts (topics), we can represent each document in a lower dimensional vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First get a tfidf BOW representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_topics = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = list(stop_words.ENGLISH_STOP_WORDS) + ['movie', 'film', 'just', 'like', 've', \"'ve\", \"'m\", \"'s\", \"'ll\", \"ll\",\n",
    "                                               'really',\n",
    "                                              ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([('tfidf', TfidfVectorizer(min_df=0.001, stop_words=stops)),\n",
    "                     ('tsvd', TruncatedSVD(n_components=num_of_topics, n_iter=10)),\n",
    "                     ('norm', Normalizer())\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pipeline.fit_transform(' '.join(sent) for sent in tri_colloc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pipeline.named_steps['tfidf']\n",
    "tsvd = pipeline.named_steps['tsvd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2501219382518581"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsvd.explained_variance_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repurposed from https://de.dariah.eu/tatom/topic_model_python.html#using-non-negative-matrix-factorization\n",
    "def print_topic_words(components, vocab, num_topics=10, num_of_word_per_topic=10):\n",
    "    topic_words = []\n",
    "    vocab_array = np.array(vocab)\n",
    "\n",
    "    for topic in components:\n",
    "        word_idx = np.argsort(topic)[::-1][:num_of_word_per_topic]\n",
    "        topic_words.append((vocab_array[word_idx]).tolist())\n",
    "        \n",
    "    for topic, words in list(zip(['Topic_{}'.format(i+1) for i in range(num_topics)], topic_words))[:10]:\n",
    "        print(topic, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic_1 ['good', 'bad', 'time', 'story', 'great', 'people', 'watch', 'movies', 'way', 'think']\n",
      "Topic_2 ['good', 'points', 'humour', 'taste', 'performances', 'scenery', 'fairly', 'story_line', 'equally', 'evil']\n",
      "Topic_3 ['bad', 'movies', 'acting', 'thing', 'script', 'worse', 'ca', 'funny', 'writing', 'plot']\n",
      "Topic_4 ['great', 'acting', 'story', 'bad', 'cast', 'actors', 'music', 'performance', 'actor', 'idea']\n",
      "Topic_5 ['time', 'great', 'bad', 'watch', 'good', 'waste', 'saw', 'watched', 'worth', 'favorite']\n",
      "Topic_6 ['watch', 'people', 'movies', 'think', 'say', 'better', 'seen', 'want', '10', 'did']\n",
      "Topic_7 ['story', 'watch', 'time', 'bad', 'great', 'acting', 'told', 'interesting', 'good', 'worth']\n",
      "Topic_8 ['seen', 'movies', 'best', 'films', 'better', 'worst', 'acting', '10', 'times', 'love']\n",
      "Topic_9 ['think', 'way', 'better', 'did', 'plot', 'make', 'characters', 'did_n', 'funny', 'end']\n",
      "Topic_10 ['think', 'story', 'seen', 'best', 'watch', 'time', 'great', 'better', 'bad', 'worst']\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(tsvd.components_, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looks like some more preprocessing is needed. I'll leave that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1187"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirchlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.cs.cornell.edu/courses/cs6784/2010sp/lecture/30-BleiEtAl03.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](LDA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_pipe = Pipeline([('tfidf', TfidfVectorizer(min_df=0.001, stop_words=stops)),\n",
    "                     ('lda', LatentDirichletAllocation(n_components=num_of_topics)),\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zacharywentzell/anaconda3/envs/lecture11/lib/python3.6/site-packages/sklearn/decomposition/online_lda.py:536: DeprecationWarning: The default value for 'learning_method' will be changed from 'online' to 'batch' in the release 0.20. This warning was introduced in 0.18.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "lda_pipe.fit_transform(' '.join(sent) for sent in tri_colloc_text)\n",
    "tfidf = lda_pipe.named_steps['tfidf']\n",
    "lda = lda_pipe.named_steps['lda']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic_1 ['wonderful', 'sad', 'likely', 'jane', 'flying', 'finding', 'finds', 'fine', 'fit', 'flat']\n",
      "Topic_2 ['way', 'going', 'performance', 'excellent', 'direction', 'cheap', 'boys', 'german', 'island', 'think']\n",
      "Topic_3 ['love', 'course', 'loved', 'remember', 'women', 'child', 'important', 'silly', 'parents', 'realistic']\n",
      "Topic_4 ['oh', 'british', 'setting', 'lady', 'surely', 'thinks', 'plan', 'pain', 'finds', 'fine']\n",
      "Topic_5 ['evil', 'dark', 'killer', 'police', 'keeps', 'mysterious', 'prison', 'soldiers', 'richard', 'government']\n",
      "Topic_6 ['hell', 'happens', 'matter', 'english', 'situation', 'meant', 'convincing', 'present', 'knowing', 'focus']\n",
      "Topic_7 ['night', 'average', 'beauty', 'casting', 'class', 'paul', 'showed', 'super', 'common', 'adds']\n",
      "Topic_8 ['hope', 'black', 'add', 'lots', 'waste', 'wasted', 'dance', 'story', 'flying', 'focus']\n",
      "Topic_9 ['dead', 'able', 'directed', 'blood', 'murder', 'question', 'suspense', 'comment', 'mystery', 'note']\n",
      "Topic_10 ['beautiful', 'unfortunately', 'dialogue', 'beginning', 'la', 'fear', 'battle', 'innocent', 'not_sure', 'poorly']\n"
     ]
    }
   ],
   "source": [
    "print_topic_words(lda.components_, tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out LDA2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://multithreaded.stitchfix.com/blog/2016/05/27/lda2vec/#topic=38&lambda=1&term="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also check out Non-negative Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://scikit-learn.org/stable/auto_examples/applications/plot_topics_extraction_with_nmf_lda.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
